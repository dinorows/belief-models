{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence scores ranking for text summarization\n",
    "This approach aims at finding the sentences that are more similar to other sentences in the text. By using sentence embedding, we can calculate the cosine distance between the sentences. We sum up the those distance and that becomes the `similarity scores` of a sentence. We assume that the highere similarity, the sentence is more likely to be a key sentence of the text. We keep the `TOP 5` result as a rough text summarization. \n",
    "\n",
    "Dino: Good job, Nan, summarizing what you are going to do in the notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "At first, we selected an article `ON TACTICS AGAINST JAPANESE IMPERIALISM`, written by Mao. As an political article, we can easily find the key sentences of the article and make summarization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44958"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ''\n",
    "\n",
    "with open('data/ON_TACTICS_AGAINST_JAPANESE_IMPERIALISM.txt', 'r') as reader:\n",
    "    text = reader.read()\n",
    "    \n",
    "    \n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44819"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# remove reference mark, e.g. '[1]'\n",
    "text = re.sub(r'\\[\\d{1,3}\\]' ,'', text)\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dino: Good idea to use `sent_tokenize` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# split into sentences\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dino: What is the difference between the `bert-base-nli-mean-tokens` model, and the `stsb-roberta-large` one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44056144,  0.5195048 ,  2.5450442 , ...,  0.5283022 ,\n",
       "        -0.46262807,  0.19722113],\n",
       "       [-0.24993764, -0.7121983 ,  1.2375451 , ..., -1.0851908 ,\n",
       "        -0.77441585,  0.10610564],\n",
       "       [-0.331048  , -0.12393019,  1.3653917 , ..., -0.72380173,\n",
       "        -0.92983496,  0.26965213],\n",
       "       ...,\n",
       "       [ 0.19709875, -0.39470232,  3.0218735 , ..., -0.4574634 ,\n",
       "        -0.34476504,  0.0278206 ],\n",
       "       [ 0.52104473, -0.29295123,  1.9758993 , ...,  0.1811067 ,\n",
       "        -0.92158616, -0.10381993],\n",
       "       [-0.7256513 ,  0.35026634,  0.09495329, ..., -0.47348344,\n",
       "        -0.92776227, -0.3470151 ]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get embedding\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "sentence_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dino: Why do you normalize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "# Noralize the data\n",
    "norm_data = normalize(sentence_embeddings, norm='l2')\n",
    "\n",
    "\n",
    "def get_sum_scores(sentence_idx):\n",
    "    \"\"\"input sentence index, get the sum of cosine distance between\n",
    "    current sentence and other sentences in the text(score)\n",
    "    @param: sentence_idx, index of sentence\n",
    "    @return score(float)\n",
    "    \"\"\"\n",
    "    scores = np.dot(norm_data, norm_data[sentence_idx].T)\n",
    "    return np.sum(scores)\n",
    "    \n",
    "    \n",
    "scores_list = []\n",
    "for i in range(len(sentences)):\n",
    "    scores_list.append(get_sum_scores(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dino: Algebraically, the dot product of two vectors is the sum of the products of the corresponding entries of the two sequences of numbers. Geometrically, it is the product of the Euclidean magnitudes of the two vectors *and* the cosine of the angle between them. It is *not* the cosine, unless you divide by the magnitude afterwards...\n",
    "\n",
    "However, since you normalized the vectors beforehand, it actually is :-)\n",
    "\n",
    "You may want to explain this in a markdown cell and motivate the reader beforehand. Otherwise, it is very painful for the reader to understand what you are doing. Your job, when you write a notebook, is to make it easy ***for the reader***, not for you. It's essentially like designing a UI. Do you mean the UI to be used by ***you***, or the rest of the world?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168.27069 195\n",
      "max scores sentence is: For all that, China's revolutionary war will remain a protracted one; this follows from the strength-of imperialism and the uneven development of the revolution.\n"
     ]
    }
   ],
   "source": [
    "# find out the max scores sentence\n",
    "max_score = 0\n",
    "max_idx = -1\n",
    "for idx, sc in enumerate(scores_list):\n",
    "    if(sc > max_score):\n",
    "        max_score = sc\n",
    "        max_idx = idx\n",
    "\n",
    "print(max_score, max_idx)\n",
    "print(\"max score sentence is: \"+ sentences[max_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dino: The calculus seems a bit careless. You want to find the sentence that is the closest match to every other sentence in the text. In other words, you are attempting to do a clustering and to find the centroid of the cluster, and then the point closest to that centroid. But you have not convinced me that this is the way to do it.\n",
    "\n",
    "To start, you need to compare *every* sentence to *every other* sentence at least (this is an N$^2$ complexity), and to me it looks like your computations above are linear in complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(167.46082, 152),\n",
       " (167.6716, 257),\n",
       " (167.67487, 87),\n",
       " (168.06612, 198),\n",
       " (168.27069, 195)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find out the top 5(min heap algorithm)\n",
    "import heapq\n",
    "\n",
    "minHeap = []\n",
    "\n",
    "for idx, sc in enumerate(scores_list):\n",
    "    if len(minHeap) == 5:\n",
    "        if(sc > minHeap[0][0]):\n",
    "            heapq.heapreplace(minHeap, (sc, idx))\n",
    "    else:\n",
    "        heapq.heappush(minHeap, (sc, idx))\n",
    "\n",
    "ans = []\n",
    "while len(minHeap) > 0 :\n",
    "    ans.append(heapq.heappop(minHeap))\n",
    "    \n",
    "ans    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whoever questions our ability to lead the revolutionary war will fall into the morass of opportunism.\n",
      "Not only are the Communist Party and the Red Army serving as the initiator of a national united front against Japan today, but in the future too they will inevitably become the powerful mainstay of China's anti-Japanese government and army, capable of preventing the Japanese imperialists and Chiang Kai-shek from carrying through their policy of disrupting this united front.\n",
      "Therefore, we emphatically assert that when the national crisis reaches a crucial point, splits will occur in the Kuomintang camp.\n",
      "But we must also say that imperialism is still a force to be earnestly reckoned with, that the unevenness in the development of the revolutionary forces is a serious weakness, and that to defeat our enemies we must be prepared to fight a protracted war; this is another characteristic of the present revolutionary situation.\n",
      "For all that, China's revolutionary war will remain a protracted one; this follows from the strength-of imperialism and the uneven development of the revolution.\n"
     ]
    }
   ],
   "source": [
    "# print out the top 5 sentences\n",
    "for sc, idx in ans:\n",
    "    print(sentences[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dino: It does not make sense to me that there be only one main point in the corpus, and that all other sentences are a variation of that point. So I don't think this approach will work. Even top-5 won't work, because its the top-5 closest to ***one*** centroid. You want different centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pretrained Summarization model\n",
    "We would like to compare our `Top 5` result to the state-of-the-art text summarization model, here we selected the pretained `T-5` model from google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ke.na/.conda/envs/pytorch/lib/python3.7/site-packages/transformers/models/auto/modeling_auto.py:762: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# using google T-5 model\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "model = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5 uses a max_length of 512 so we cut the article to 512 tokens.\n",
    "inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate summary\n",
    "outputs = model.generate(inputs, max_length=180, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> a great change has now taken place in the political situation in china. it's main characteristic is that Japanese imperialism wants to turn China into a colony. the workers and the peasants are all demanding resistance.</s>\n"
     ]
    }
   ],
   "source": [
    "# decode the summary\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained summarization model for top 5 rank sentence\n",
    "What about we using summariztion model to summarize the ONLY top-5 sentence? Will it give us more concise result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Whoever questions our ability to lead the revolutionary war will fall into the morass of opportunism.Not only are the Communist Party and the Red Army serving as the initiator of a national united front against Japan today, but in the future too they will inevitably become the powerful mainstay of China's anti-Japanese government and army, capable of preventing the Japanese imperialists and Chiang Kai-shek from carrying through their policy of disrupting this united front.Therefore, we emphatically assert that when the national crisis reaches a crucial point, splits will occur in the Kuomintang camp.But we must also say that imperialism is still a force to be earnestly reckoned with, that the unevenness in the development of the revolutionary forces is a serious weakness, and that to defeat our enemies we must be prepared to fight a protracted war; this is another characteristic of the present revolutionary situation.For all that, China's revolutionary war will remain a protracted one; this follows from the strength-of imperialism and the uneven development of the revolution.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5 = ''.join([sentences[idx] for sc, idx in ans])\n",
    "\n",
    "top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ke.na/.conda/envs/pytorch/lib/python3.7/site-packages/transformers/models/auto/modeling_auto.py:762: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# using T5 model\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "model_top5 = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
    "tokenizer_top5 = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "inputs_top5 = tokenizer_top5.encode(\"summarize: \" + top5, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "outputs_top5 = model_top5.generate(inputs_top5, max_length=180, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> china's revolutionary war will remain a protracted one. uneven development of revolutionary forces is a serious weakness. china's imperialism is still a force to be earnestly reckoned with.</s>\n"
     ]
    }
   ],
   "source": [
    "## summary of the top-5 sentences\n",
    "print(tokenizer_top5.decode(outputs_top5[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization model test -- A fail example\n",
    "The pretained model is powerful, but not always working, below is an example: we selected the Mao's wiki, when summarize the the wikipedia(text in chronological order), the model can hardly catch the whole map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31284"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_wiki = ''\n",
    "\n",
    "with open('data/mao_wiki.txt', 'r') as reader:\n",
    "    text_wiki = reader.read()\n",
    "    \n",
    "    \n",
    "len(text_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30645"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# remove reference mark, e.g. '[1]'\n",
    "text_wiki = re.sub(r'\\[\\d{1,4}\\]' ,'', text_wiki)\n",
    "len(text_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ke.na/.conda/envs/pytorch/lib/python3.7/site-packages/transformers/models/auto/modeling_auto.py:762: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# using T5 model to summarize\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "model_wiki = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
    "tokenizer_wiki = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "inputs_wiki = tokenizer_wiki.encode(\"summarize: \" + text_wiki, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "outputs_wiki = model_wiki.generate(inputs_wiki, max_length=180, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> 1917â€“19 Mao moved to Beijing, where his mentor Yang Changji took a job at peking university. he was snubbed by other students due to his rural Hunanese accent and lowly position. he joined Li's Study Group and \"developed rapidly toward Marxism\" during the winter of 1919.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_wiki.decode(outputs_wiki[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Top-5 model for wiki\n",
    "We also would like to test our `Top 5` model for the wikipedia article, and compare the result with the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# split into sentences\n",
    "\n",
    "sentences = sent_tokenize(text_wiki)\n",
    "\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10726409,  1.0987113 , -0.09853894, ...,  0.21398848,\n",
       "        -0.18605344,  0.4299076 ],\n",
       "       [-1.0969067 ,  0.87336105,  0.07322763, ...,  0.22656934,\n",
       "        -0.00875739, -0.26058328],\n",
       "       [-0.59271616,  0.1364042 ,  0.17228283, ..., -0.5263024 ,\n",
       "        -0.15850013,  0.5666151 ],\n",
       "       ...,\n",
       "       [ 0.25065625,  0.23724607, -0.04016513, ...,  0.4672328 ,\n",
       "         1.4903196 , -0.24578984],\n",
       "       [-0.7199149 ,  0.5134689 , -0.61556673, ..., -0.08170696,\n",
       "         0.10948969,  0.47148797],\n",
       "       [-0.34347   ,  1.011436  , -0.07794081, ...,  0.09482689,\n",
       "         0.4004276 , -0.04142505]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "# Noralize the data\n",
    "norm_data = normalize(sentence_embeddings, norm='l2')\n",
    "\n",
    "\n",
    "def get_sum_scores(sentence_idx):\n",
    "    \"\"\"input sentence index, get the sum of scores\n",
    "    @param: sentence_idx, index of sentence\n",
    "    @return scores(float)\n",
    "    \"\"\"\n",
    "    scores = np.dot(norm_data, norm_data[sentence_idx].T)\n",
    "    return np.sum(scores)\n",
    "    \n",
    "    \n",
    "scores_list = []\n",
    "for i in range(len(sentences)):\n",
    "    scores_list.append(get_sum_scores(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These demonstrations ignited the nationwide May Fourth Movement and fueled the New Culture Movement which blamed China's diplomatic defeats on social and cultural backwardness.\n",
      "In the winter of 1925, Mao fled to Guangzhou after his revolutionary activities attracted the attention of Zhao's regional authorities.\n",
      "Although Chiang intended to ignore Mao's message and continue the civil war, he was arrested by one of his own generals, Zhang Xueliang, in Xi'an, leading to the Xi'an Incident; Zhang forced Chiang to discuss the issue with the Communists, resulting in the formation of a United Front with concessions on both sides on December 25, 1937.\n",
      "In December 1919, Mao helped organise a general strike in Hunan, securing some concessions, but Mao and other student leaders felt threatened by Zhang, and Mao returned to Beijing, visiting the terminally ill Yang Changji.\n",
      "Civil War\n",
      "Main articles: Chinese Civil War and Chinese Communist Revolution\n",
      "The Nanchang and Autumn Harvest Uprisings: 1927\n",
      "\n",
      "Flag of the Chinese Workers' and Peasants' Red Army\n",
      "Fresh from the success of the Northern Expedition against the warlords, Chiang turned on the Communists, who by now numbered in the tens of thousands across China.\n"
     ]
    }
   ],
   "source": [
    "# top 5\n",
    "import heapq\n",
    "\n",
    "minHeap = []\n",
    "\n",
    "for idx, sc in enumerate(scores_list):\n",
    "    if len(minHeap) == 5:\n",
    "        if(sc > minHeap[0][0]):\n",
    "            heapq.heapreplace(minHeap, (sc, idx))\n",
    "    else:\n",
    "        heapq.heappush(minHeap, (sc, idx))\n",
    "\n",
    "ans = []\n",
    "while len(minHeap) > 0 :\n",
    "    ans.append(heapq.heappop(minHeap))\n",
    "    \n",
    "for sc, idx in ans:\n",
    "    print(sentences[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarization Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State of the art model: Pegasus, see: https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html\n",
    "\n",
    "In 2021 Google IO, google annouced LaMDA platform for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dino: I think we need a better understanding of the SOTA of text summarization. I doubt that there's only one supervized approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
