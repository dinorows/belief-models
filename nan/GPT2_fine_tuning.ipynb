{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../mao/REPORT ON AN INVESTIGATION OF THE PEASANT MOVEMENT IN HUNAN.txt\n",
      "../mao/ON CORRECTING MISTAKEN IDEAS IN THE PARTY.txt\n",
      "../mao/BE CONCERNED WITH THE WELL-BEING OF THE MASSES, PAY ATTENTION TO METHODS OF WORK.txt\n",
      "../mao/WHY IS IT THAT RED POLITICAL POWER CAN EXIST IN CHINA.txt\n",
      "../mao/A SINGLE SPARK CAN START A PRAIRIE FIRE.txt\n",
      "../mao/ANALYSIS OF THE CLASSES IN CHINESE SOCIETY.txt\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "article = []\n",
    "# titles\n",
    "titles = []\n",
    "\n",
    "import os\n",
    "for root, dirs, files in os.walk(\"../mao/\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            print(os.path.join(root, file))\n",
    "            with open(os.path.join(root, file), \"r\") as input:\n",
    "                sentences = input.read()\n",
    "            article.append(sentences)\n",
    "            titles.append(file)\n",
    "            \n",
    "print(len(article))\n",
    "print(len(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate text\n",
    "text = ''.join(article)\n",
    "\n",
    "len(text)\n",
    "# remove reference mark, e.g. '[1]'\n",
    "\n",
    "import re\n",
    "\n",
    "text = re.sub(r'\\[\\d{1,3}\\]' ,'', text)\n",
    "len(text)\n",
    "# split inot sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium').cuda()\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Here's his first full trailer!\n",
      "\n",
      "This clip of the upcoming episode of \"Star Trek\" which I have posted over will show him from time to time until he drops another teaser for the episode which will take the action scene right in our lives as soon as the movie makes this trip right around the corner which doesn't take toooo long! Hope this post makes your weekend much more enjoyable, I'm totally looking forward to it (especially after my week at a comedy show!) If there's something new for \"Archer Season 3\"—maybe new CGI or even something new that is going over well with both us readers or Trekkies—\"Fever\" fans—and that something else that does \"good\" with Star Trek is being pitched, it's definitely going to do fine. This doesn't mean we have to stay glued watching the Trek channel for every new film, but let's hope Star Trek season 8 is something very worthwhile to be looking forward to all those years for fans! Let The Klingons Have An Apportionment for the Uplift! And while everything up to that has to work their way into our hearts before coming the year we meet our newest star on the BIG Screen is all \"just in time for Halloween, and also all too fitting\". This would be our week! The Starlog has a \"Spacing\" post up now: \"Spacerature is one of those weirdly exciting ideas the TV market never forgets, even when they've made\n",
      "1: This report describes three steps a company undertook in 2009 to limit its vulnerability, including how this impacted its performance and the extent and outcomes of its vulnerabilities over and above baseline reviews conducted over recent years.\n",
      "2: What is a tax and who is it for, your employer or government?\n",
      "\n",
      "Many businesses don't collect sales taxes but do charge certain services; property (pre-payment) taxes etc, all these fees typically come up due time before a job comes on the balance sheets. An important part of an invoice is whether the invoice will have to be recorded as a credit, a penalty etc under Section 42 or 54, as the law provides for both these. So you won't lose any income tax if that does go due and your contract says you will file a record of service, however a credit can go off on a charge of 'purchase price', as may have previously been paid for services before you went on offer for employment. Your credit needs to be registered on a document (preparation for credit is where the extra invoice goes that usually records it in terms of GST and would result either way a refund due. You will lose out if you have a charge, e.g a small business may have just gone about 'business as usual'. A charge of purchase is only necessary for credit if no other agreement takes payment of the tax credit, so normally any services in fact sold do come within your agreement with your business. A service such one-off, however, in terms, it may not need registering. In our experience'special' credit often is more than the minimum you charge. Also of course for this payment to include any taxes paid the person may not know it\n",
      "3: While still in middle-of-the-river territory these guys are showing signs of taking another risk (atleast outside of San Diego), it doesn't seem to mean their roster situation can easily slide over this weekend and get down to Miami for games as tight as this at FedEx Field (atleast a 10 point and overtime lead will likely see Florida on the road for four road games) and Boston for the final two meetings ahead of its showdown here on Saturday in Philadelphia as Ohio State hosts Penn State next weekend ahead of Florida State which travels for this week-to-play game versus Ohio State and also games at Notre Dame while playing in the Chick-fil-A Kickoff.\n",
      "\n",
      "But that hasn't dampened their fervor any given day in Orlando since they came up empty against USC with the win or when the Wolverhampton forward was the starter as well back at UCLA two years ago when he started off in place after freshman Matt Dolecat played a late minutes over 20 minute period to win as coach of the Orange. Just four seasons earlier Michigan Tech and Villanova shared another one of these moments that was memorable, after the Wolverhampton striker's last kick to finish a four touch match up he put all confidence to use going five clear off eight scoring in the 75th and 77. After that it's another back straight for the forward as this was not on record just yet against a defense in transition to get a score that's about a touchdown.\n",
      "\n",
      "4: This month was very interesting. There had only been 8 meetings from the 3 participants from November 2013 until February and there were almost 10 sessions. Some were really intense!\n",
      "One session, my favorite and most important was with Jon Ransom who, by his very definition and his very opinion (not yours truly, please). Jon reminded me that you can't really trust somebody who hasn't served me before on the subject: You must first try, and to trust that's an act a better to make you understand someone, and a wiser as regards that's just that they are more human. That is the most important of lessons in life… The thing is, my previous experience taught me another… and that too must come after the time I lost it was too soon! Anyway there weren't many other speakers… well no more there than usual and that's always an easy fact on many conferences. We also had two presentations, so another good indicator on what you might get on February 7th was some meetings in a year. Anyway we didn´T meet every single day – only on most in November/March/June 2014 where Jon would go around his room giving various tips etc. Anyway they all ended for most the second week… and there he had come the night before (of February 2) to talk us through how to fix every little fix we came to because of how badly we have to adjust this \"unfinished work\"? I couldn´t get around to the one from January so\n",
      "5: It's a simple one. And not too small! To help my small children play while I'm driving from home, they play in two tiny cars under water until about 12 mph, when one of my sons gets out again into his blue Volkswagen hatchback (he uses ours for his regular errands). I always find him out around 5-6 miles on either end when I start up again. This may even have to stay longer than an hour. You know when I tell him his name because as the little thing approaches close quarters his voice becomes louder than on mine because he's feeling it more while he's playing. There it came: The word of the Day…in no easy terms; BUT I found some other awesome ways my children played when and what we decided in \"The Secret Key!!\" What do I mean this TIME!?\n",
      "\n",
      "Just imagine you are trying to explain an imaginary story and the subject matters can start all across the room (or, like a child doing a crossword.) One of all sorts; the word (there have to have only two here — A story-of.) and why can NOT explain, \"We just wanted someone like Jellica in their corner on their birthday to tell her and she turned herself in\"? A simple question, \"what was to blame if things didn't progress as advertised?\" The other thing you are being told, (you didn't like that particular person – sorry) might be to say \"oh that person,\n",
      "6: The Obama Foundation had previously claimed on their website that in the \"early months of 2014\" Clinton signed onto a $250 billion fundraising pledge \"without campaign commitment from anyone.\" In 2005 however documents appeared confirming Obama made only $225 million \"out of a $15 million donation goal.\"\n",
      "\n",
      "\n",
      "On Hillarys 'Gifted Hand,\" I documented this in 2005 before Donald Trump became our party boss https://t.co/cG8qJ7U0QX — Bill O'Reily(*) (@Billocruz726) April 22, 2015\n",
      "\n",
      "That the Obama Family is so rich was a given from 2012\n",
      "\n",
      "After leaving politics in 2016, and with his family assets in line with Clinton Cash, I wrote a piece published last April saying Obama's wealth began dropping after leaving office, when Bill & Hillary made significant gains via a lucrative practice of giving huge amounts at charity fundraisers (which had to be disclosed under campaign finance rules.)\n",
      "\n",
      "Hillary didn't pay Hillary $100k for hosting a 'victim benefit speech'. Obama hosted her speech with Clinton Foundation lawyers @JillianDCarr\n",
      "\n",
      "It's also obvious Obama had huge problems paying out charitable money for \"aid, reconstruction and development\" and instead decided he needed \"tax-exempt and community services contracts…\"\n",
      "\n",
      "That this is the problem Obama needs resolved: Trump and Ted Cruz\n",
      "\n",
      "\n",
      "Here @foxandfriends claims President Obama called Trump the worst potential president the world ever produced http://\n",
      "7: Here's an important information for all players\n",
      "\n",
      "Since January 2015 no player that is free is available to enter and sign during World Championships Qualifiers. During tournaments with teams or in cases not sanctioned the option is removed for this league as per World Championships guidelines – unless in tournaments with teams – in this article I give more details including this link but its pretty clear that the option is removed and any free registration from outside of official league that uses team qualifiers or league tournaments that do not have a qualifier for players from Russia is not allowed at these locations.\n",
      "8: BANGKI, Laos\n",
      "\n",
      "The air and space forces of Laos would be much more formidable when they can be protected from their enemy on the skies overhead as part of a proposed \"superfly zone' under new national leadership. It says the force - now officially called DZ - of nearly 1.2 million people — which includes a fleet of 20 MiGs — are as capable of defending themselves as American aircraft flying all to the west.\n",
      "\n",
      "\n",
      "But that has not deterred American politicians who would like to use Laos to set up some regional air defense — the idea is of using small, medium sized and advanced MiG's of its size or better against Chinese warfighting to ensure the safety of their own troops. For this to make some kind of sense we do have the capability to create one as such - the world's smallest - - and create the Air Combat Command. The proposal would take advantage of an existing Air Combat Center in India and have Air Force units land anywhere on earth on the most-modern equipment.\n",
      "\n",
      "\n",
      "\"What this capability really signifies, if this is taken away here or here in Cambodia (a nation much smarter that Laos — you can tell they understand math and the universe are about 8.7 -9 -plus... therefor have been prepared to put you up to an arm of some real aircraft... because the math seems a lot lighter\"), it says is taking away one more tool which could ensure American lives at ground level should their air dominance prove challenged\n",
      "9: From Star Trek Online Wiki\n",
      "\n",
      "These Star Systems require a specific type of fleet member vessel that can make space for both their own fleet as and to fly for those ships, using different types of powergrid options. For comparison on available options and examples of use in the examples and guides that this system also serves an element of PvP role. The different examples show what different powertracked space based space of this ship can make to those pilots.\n",
      "\n",
      "This specific fleet ship makes space, with its base level, for both a Klingon Navy Farsheep-class Frigate and the Uxdeel fleet that accompanies this faction at high quality/value costs which includes one VE/S (Coupon) or PVP (Class-6-Type) warp bridge. Unlike a common fleet space at other players can fit ships for as low for any player it can fit at maximum efficiency to their own faction within 5x its initial cost. A player choosing such a space may choose to fill in the gap, or fill both its own (Khan) Federation/Klingon-provided fleet ships when they can make more.\n",
      "\n",
      "Examples for both fleets will help further elucidate which is appropriate for players to use more or leave to some other space.\n",
      "\n",
      "\n",
      "See also [ to look for more of our Fleet space recommendations\n",
      ", you must be level 38+ before you use all your Fleet space within 3 hours of the start of each week - please contact our support\n"
     ]
    }
   ],
   "source": [
    "# generate 10 sentences for testing\n",
    "generated = tokenizer(\"<|endoftext|>\", return_tensors=\"pt\").input_ids.cuda()\n",
    "sample_outputs = model.generate(generated, do_sample=True, top_k=50, \n",
    "                                max_length=300, top_p=0.95, temperature=1.9, num_return_sequences=10)\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 1024)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium', bos_token='<|startoftext|>',\n",
    "                                          eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium').cuda()\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(tokenizer.encode(s)) for s in sentences])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, max_length):\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "        for txt in txt_list:\n",
    "            encodings_dict = tokenizer('<|startoftext|>' + txt + '<|endoftext|>', truncation=True,\n",
    "                                       max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(sentences, tokenizer, max_length=max_length)\n",
    "# train test split\n",
    "train_size = int(0.9 * len(dataset))\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir='./results', num_train_epochs=2, logging_steps=100, save_steps=5000,\n",
    "                                  per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "                                  warmup_steps=10, weight_decay=0.05, logging_dir='./logs', report_to = 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2250/2250 11:09, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.989100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.501100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.485400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.436500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.456000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.434200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.456900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.432700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.409500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.457500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.451300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.341400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.290300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.292400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.337400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.263000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.303100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.297000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.347700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.307600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.252300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2250, training_loss=0.4445423355102539, metrics={'train_runtime': 670.2388, 'train_samples_per_second': 3.357, 'total_flos': 1034670329856000.0, 'epoch': 2.0, 'init_mem_cpu_alloc_delta': 8192, 'init_mem_gpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 4395008, 'train_mem_gpu_alloc_delta': 4265118208, 'train_mem_cpu_peaked_delta': 0, 'train_mem_gpu_peaked_delta': 1096393728})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer(model=model,  args=training_args, train_dataset=train_dataset, \n",
    "        eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
    "                                                              'attention_mask': torch.stack([f[1] for f in data]),\n",
    "                                                              'labels': torch.stack([f[0] for f in data])}).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  为弙遊丝 一本大一乳啊亚充俘 中區行勝克?\n",
      "1:   The three months' worth per capita rent are five denarii ten denars.\n",
      "2:  When we call for donations to be carried out on a production, our purpose would very rarely be any other than to be supplied to our Red Army from Red funds, that is, money and hard work.\n",
      "3: .\n",
      "4: ?\n",
      "5:   The rural women refuse to yield to the city gentry  which puts forward the demand, \"Give us women's right!\" \n",
      "6:  The most notorious crimes were being carried out in rural areas and other places, when rural areas could not cope with the population.\n",
      "7:  **************************** B. ROWL\n",
      "Though these revolutionary elements now under Kiangsi military control also number comparatively small and comprise only 30-40 per cent of the proletariat., in the old regime the overwhelming majority, in places as far as Pao Tsuochua from Kao'o County to the Wuen Chuan were peasant revolutionary fronts; the remaining 1 per cent tended to join the \"road work\" or petty-military workers  of the Kiangsi garrison-peasant associations, which for economic struggle was quite numerous but relatively powerless at least then.\n",
      "8:   and, in contrast, some of our comrades would deny that the peasantry exists, say or do very bad or even heinous things; at every possible opportunity we give our Party an idea about a \"revolution in short motion and great motion\".\n",
      "9:   They are particularly skittish on seeing women and small children, because they have no faith in the idea that they need anybody,  least on behalf as their equal  in life, though only on account they suffer so greatly more from the narrow sphere of being different and therefore prefer being closer in spirit and outlook.\n"
     ]
    }
   ],
   "source": [
    "# test model \n",
    "generated = tokenizer(\"<|startoftext|> \", return_tensors=\"pt\").input_ids.cuda()\n",
    "sample_outputs = model.generate(generated, do_sample=True, top_k=50, \n",
    "                                max_length=300, top_p=0.95, temperature=1.9, num_return_sequences=10)\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
