{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../mao/REPORT ON AN INVESTIGATION OF THE PEASANT MOVEMENT IN HUNAN.txt\n",
      "../mao/ON CORRECTING MISTAKEN IDEAS IN THE PARTY.txt\n",
      "../mao/BE CONCERNED WITH THE WELL-BEING OF THE MASSES, PAY ATTENTION TO METHODS OF WORK.txt\n",
      "../mao/WHY IS IT THAT RED POLITICAL POWER CAN EXIST IN CHINA.txt\n",
      "../mao/A SINGLE SPARK CAN START A PRAIRIE FIRE.txt\n",
      "../mao/ANALYSIS OF THE CLASSES IN CHINESE SOCIETY.txt\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "article = []\n",
    "# titles\n",
    "titles = []\n",
    "\n",
    "import os\n",
    "for root, dirs, files in os.walk(\"../mao/\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            print(os.path.join(root, file))\n",
    "            with open(os.path.join(root, file), \"r\") as input:\n",
    "                sentences = input.read()\n",
    "            article.append(sentences)\n",
    "            titles.append(file)\n",
    "            \n",
    "print(len(article))\n",
    "print(len(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate text\n",
    "text = ''.join(article)\n",
    "\n",
    "len(text)\n",
    "# remove reference mark, e.g. '[1]'\n",
    "\n",
    "import re\n",
    "\n",
    "text = re.sub(r'\\[\\d{1,3}\\]' ,'', text)\n",
    "len(text)\n",
    "# split inot sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874baa7db68947efaf28411cf0f0e7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1042301.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2050ffa19343a19cc928debeba6b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=456318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0034b070783d447fa9d7bff8fc8475a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1355256.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca83d5cbda2746f1ba4bcb455752b9ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=718.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b235261d42f542e0ab44ea509e63039a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1520013706.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 1024)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium', bos_token='<|startoftext|>',\n",
    "                                          eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium').cuda()\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(tokenizer.encode(s)) for s in sentences])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, max_length):\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "        for txt in txt_list:\n",
    "            encodings_dict = tokenizer('<|startoftext|>' + txt + '<|endoftext|>', truncation=True,\n",
    "                                       max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(sentences, tokenizer, max_length=max_length)\n",
    "# train test split\n",
    "train_size = int(0.9 * len(dataset))\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir='./results', num_train_epochs=1, logging_steps=100, save_steps=5000,\n",
    "                                  per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "                                  warmup_steps=10, weight_decay=0.05, logging_dir='./logs', report_to = 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1125/1125 06:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.382700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.386500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.466800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.499700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.546800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.423000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.411700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.438000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.474300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.399800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.369700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1125, training_loss=0.43550369177924264, metrics={'train_runtime': 387.2959, 'train_samples_per_second': 2.905, 'total_flos': 517335164928000.0, 'epoch': 1.0, 'init_mem_cpu_alloc_delta': 20480, 'init_mem_gpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 1486848, 'train_mem_gpu_alloc_delta': 2838608384, 'train_mem_cpu_peaked_delta': 0, 'train_mem_gpu_peaked_delta': 1101722624})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer(model=model,  args=training_args, train_dataset=train_dataset, \n",
    "        eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
    "                                                              'attention_mask': torch.stack([f[1] for f in data]),\n",
    "                                                              'labels': torch.stack([f[0] for f in data])}).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  --the whole economic structure in Japan now resembles to that  of the revolutionary countries, and even it does not take into account that a revolutionary revolution may exist where not two revolutionary units do or what is generally believed to be wrong.\n",
      "1: , and were also killed and left the carcasses outside the houses on their feet like their cattle  of meat,  a symbol being the method of punishment by making up what we call this difference--so far from slaughter, there being equal conditions inside.\n",
      "2:    a few days hence  I will ask them whether their objections are adequate.\n",
      "3:  When your work is only \"finished\", you must pay its duties for the sake only- that to please your comrades. When you have a work out, you do not take it to such an origin of importance for pleasure.\n",
      "4:  \n",
      "5: .\n",
      "6:    This also makes \"puttings for the county head \" unnecessary.\n",
      "7:  But some local tyrants and evil gentry and evil-minded landlords have also decided to commit such atrocities.\n",
      "8:    The peasants make themselves uneasy, and make many difficulties. The local governments are alarmed.\n",
      "9:  Therefore we must all be against being idle.\n"
     ]
    }
   ],
   "source": [
    "# test model \n",
    "generated = tokenizer(\"<|startoftext|> \", return_tensors=\"pt\").input_ids.cuda()\n",
    "sample_outputs = model.generate(generated, do_sample=True, top_k=50, \n",
    "                                max_length=300, top_p=0.95, temperature=1.9, num_return_sequences=10)\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
